{ 
  "demo/src/main/anotherDAG.py ": 
{"imports":["os","time","uuid","pendulum","airflow.DAG","googleapiclient.discovery.build","airflow.exceptions.AirflowFailException","airflow.models.XCom","airflow.operators.bash.BashOperator","airflow.operators.empty.EmptyOperator","airflow.operators.python.PythonOperator","airflow.operators.python.BranchPythonOperator","airflow.providers.google.cloud.operators.dataflow.DataflowTemplatedJobStartOperator","airflow.providers.google.cloud.operators.gcs.GCSListObjectsOperator","airflow.utils.db.provide_session","airflow.utils.trigger_rule.TriggerRule","googleapiclient.errors.HttpError","google.cloud.monitoring_v3","pnr_dag_vars.*","pnr_vars.*","dag_utils.group_partitions_per_hour","dag_utils.select_files","google.cloud.bigquery","google.auth._default","google.auth.impersonated_credentials","google.cloud.storage","google.auth._default","google.auth.impersonated_credentials"],"classes":[],"functions":[{"name":"_get_df_service","returnType":"","parameters":[],"body":"def _get_df_service():\n    df_service = build(\"dataflow\", \"v1b3\", cache_discovery=False)\n    logging.info(\"df service [{}]\".format(df_service))\n    return df_service","annotations":[]},{"name":"list_jobs","returnType":"","parameters":[],"body":"def list_jobs():\n    logging.info(f\"Listing Dataflow jobs\")\n    df_service = _get_df_service()\n    response = (\n        df_service.projects()\n        .locations()\n        .jobs()\n        .list(projectId=PIPELINE_PROJECT_ID, location=DATAFLOW_REGION)\n        .execute()\n    )\n    jobs = response[\"jobs\"]\n    return jobs","annotations":[]},{"name":"get_job_id","returnType":"","parameters":["job_name"],"body":"def get_job_id(job_name):\n    logging.info(job_name)\n    jobs = list_jobs()\n    logging.info(\"Jobs retrieved from dataflow are [{}]\".format(jobs))\n    job_id = None\n    for job in jobs:\n        job_nm = job[\"name\"]\n        if job_nm.startswith(job_name) and job[\"currentState\"] == \"JOB_STATE_DONE\":\n            job_id = job[\"id\"]\n            logging.info(\"job_id {} for current job {}\".format(job_id, job_name))\n            break;\n        else:\n            job_id= None\n    return job_id","annotations":[]},{"name":"get_job_status","returnType":"","parameters":["job_name"],"body":"def get_job_status(job_name):\n    logging.info(job_name)\n    jobs = list_jobs()\n    job_status = None\n    for job in jobs:\n        job_nm = job[\"name\"]\n        if job_nm.startswith(job_name):\n            logging.info(job[\"currentState\"])\n            job_status = job[\"currentState\"]\n            break;\n        else:\n            job_status= None\n    return job_status","annotations":[]},{"name":"get_job_metrics","returnType":"","parameters":["job_id"],"body":"def get_job_metrics(job_id):\n    df_service = _get_df_service()\n    response = (\n        df_service.projects()\n        .locations()\n        .jobs()\n        .getMetrics(projectId=PIPELINE_PROJECT_ID, location=DATAFLOW_REGION, jobId=job_id)\n        .execute()\n    )\n    metrics = response\n    return metrics","annotations":[]},{"name":"get_valid_record_count","returnType":"","parameters":["job_name","metrics_names"],"body":"def get_valid_record_count(job_name,metrics_names):\n    logging.info(\"job_name is {}\".format(job_name))\n    job_id = get_job_id(job_name)\n    if job_id is not None:\n        logging.info(\"job_id is {}\".format(job_id))\n        job_metrics = get_job_metrics(job_id)\n        if job_metrics is None or job_metrics[\"metrics\"] is None:\n            logging.info(\"There are no metrics associated with this job {}\".format(job_name))\n        else:\n            valid_record_count = 0\n            for job_metric in job_metrics[\"metrics\"]:\n                if job_metric['name']['name'] in metrics_names:\n                    valid_record_count = job_metric['scalar']\n                    logging.info(\"job_metric {}\".format(job_metric))\n                    return valid_record_count\n    else:\n        return 0","annotations":[]},{"name":"_get_default_or_from_dag_run","returnType":"","parameters":["default_value","dag_key","dag_run_conf"],"body":"def _get_default_or_from_dag_run(default_value: str, dag_key, dag_run_conf):\n    if dag_run_conf is not None and dag_key in dag_run_conf:\n        return dag_run_conf[dag_key]\n    return default_value","annotations":[]},{"name":"get_metric_service_client","returnType":"","parameters":["target_service_account"],"body":"def get_metric_service_client(target_service_account: str):\n    target_scopes = ['https://www.googleapis.com/auth/cloud-platform']\n    return monitoring_v3.MetricServiceClient()","annotations":[]},{"name":"create_custom_metrics","returnType":"","parameters":["metric_type","project_id","resource_type","value_type","val"],"body":"def create_custom_metrics(metric_type: str, project_id: str, resource_type: str, value_type: str, val):\n    try:\n        client = get_metric_service_client(COMPOSER_SERVICE_ACCOUNT)\n        project_name = f\"projects/{project_id}\"\n        series = monitoring_v3.TimeSeries()\n        series.metric.type = CUSTOM_METRIC_DOMAIN + \"/\" + metric_type\n        series.metric.labels['application_name'] = DAGID\n        series.metric.labels['workflow_name'] = DAGID\n        series.resource.type = resource_type\n        series.resource.labels[\"project_id\"] = project_id\n        series.resource.labels[\"workflow_name\"] = DAGID\n        series.resource.labels[\"location\"] = \"us-central1\"\n        now = time.time()\n        seconds = int(now)\n        nanos = int((now - seconds) * 10 ** 9)\n        interval = monitoring_v3.TimeInterval(\n            {\"end_time\": {\"seconds\": seconds, \"nanos\": nanos}}\n        )\n        point = monitoring_v3.Point({\"interval\": interval, \"value\": {value_type: val}})\n        series.points = [point]\n        client.create_time_series(name=project_name, time_series=[series])\n    except Exception as ex:\n        logging.error(\"Error [{}] occurred while inserting/creating custom metric\".format(str(ex)))","annotations":[]},{"name":"get_bq_impersonated_client","returnType":"","parameters":["target_service_account","target_project"],"body":"def get_bq_impersonated_client(target_service_account: str, target_project: str):\n    target_scopes = ['https://www.googleapis.com/auth/cloud-platform',\n                     'https://www.googleapis.com/auth/bigquery']\n    return bigquery.Client(project=target_project, credentials=getImpersonatedCredentials(target_scopes, target_service_account, target_project))","annotations":[]},{"name":"getImpersonatedCredentials","returnType":"","parameters":["target_scopes","target_service_account","target_project"],"body":"def getImpersonatedCredentials(target_scopes: list, target_service_account: str, target_project: str):\n    from google.auth import _default\n    source_credentials, project_id = _default.default(scopes=target_scopes)\n\n    from google.auth import impersonated_credentials\n    target_credentials = impersonated_credentials.Credentials(\n        source_credentials=source_credentials,\n        target_principal=f'{target_service_account}@{target_project}.iam.gserviceaccount.com',\n        target_scopes=target_scopes,\n        lifetime=600)\n    return target_credentials","annotations":[]},{"name":"_get_storage_client","returnType":"","parameters":["target_service_account","target_project"],"body":"def _get_storage_client(target_service_account: str, target_project: str):\n    from google.cloud import storage\n    target_scopes = ['https://www.googleapis.com/auth/cloud-platform']\n    credentials = None if TEST_RUN == \"true\" else _get_impersonated_credentials(target_scopes,\n                                                                                target_service_account)\n    return storage.Client(project=target_project,\n                          credentials=credentials)","annotations":[]},{"name":"_get_impersonated_credentials","returnType":"","parameters":["target_scopes","target_service_account"],"body":"def _get_impersonated_credentials(target_scopes: list, target_service_account: str):\n    from google.auth import _default\n    source_credentials, project_id = _default.default(scopes=target_scopes)\n    logging.info(f\"Source credentials generated for project [{project_id}]\")\n\n    from google.auth import impersonated_credentials\n    target_credentials = impersonated_credentials.Credentials(\n        source_credentials=source_credentials,\n        target_principal=target_service_account,\n        target_scopes=target_scopes,\n        lifetime=60 * 60)\n    return target_credentials","annotations":[]},{"name":"_move_blobs","returnType":"","parameters":["bucket_name","blob_names","destination_bucket_name","destination_prefix"],"body":"def _move_blobs(bucket_name: str, blob_names: list, destination_bucket_name: str,\n                destination_prefix: str, **context):\n    logging.info(f\"Moving blobs from [{bucket_name}] to [{destination_bucket_name}/{destination_prefix}]\")\n    num_blobs = len(blob_names)\n    time_millis = round(time.time() * 1000)\n    tmp_file = f\"/tmp/pnr{SUFFIX}_file_copy_{time_millis}.txt\"\n    with open(tmp_file, \"a\") as f:\n        for i, blob_name in enumerate(blob_names):\n            logging.info(\n                f\"[{i + 1}/{num_blobs}] Adding blob [{bucket_name}/{blob_name}]...\")\n            f.writelines(f\"gs://{bucket_name}/{blob_name}\\n\")\n\n    impersonation_opts = \"\" if TEST_RUN == \"true\" else f\"-i {DATAFLOW_SERVICE_ACCOUNT}\"\n    gsutil_path = \"/google-cloud-sdk/bin/\" if TEST_RUN == \"true\" else \"\"\n    gsutil_state_opts = f\"-o 'GSUtil:state_dir=/tmp/pnr{SUFFIX}_gsutil_state_{time_millis}'\"\n    bash_cmd = f\"cat {tmp_file} | {gsutil_path}gsutil {gsutil_state_opts} {impersonation_opts} -m mv -I 'gs://{destination_bucket_name}/{destination_prefix}/'\"\n    logging.info(f\"Executing Bash command [{bash_cmd}]\")\n    file_copy_using_gsutil = BashOperator(\n        task_id=\"file_copy_using_gsutil\" + str(uuid.uuid4()),\n        bash_command=bash_cmd\n    )\n    file_copy_using_gsutil.execute(context)\n    os.remove(tmp_file)\n\n    logging.info(f\"Moved blobs from file [{tmp_file}] to [{destination_bucket_name}/{destination_prefix}]\")","annotations":[]},{"name":"_setup_processing","returnType":"","parameters":[],"body":"def _setup_processing(**context):\n    logging.info(f\"Starting DAG [{DAGID}-{RELEASE_RAW}]\")\n    logging.info(\"Preparing partition\")\n    logging.info(f\"Context [{context}]\")\n    execution_dt_utc = context[\"logical_date\"]\n    logging.info(f\"Execution datetime [{execution_dt_utc}] UTC\")\n\n    task_instance = context[\"task_instance\"]\n    dag_run = context[\"dag_run\"]\n    dag_run_conf = dag_run.conf if dag_run else None\n\n    gcs_landing_bucket_name = _get_default_or_from_dag_run(GCS_LANDING_BUCKET_NAME, DAG_PARAM_GCS_LANDING_BUCKET_NAME,\n                                                           dag_run_conf)\n    gcs_landing_bucket_path = _get_default_or_from_dag_run(GCS_LANDING_BUCKET_PATH, DAG_PARAM_GCS_LANDING_BUCKET_PATH,\n                                                           dag_run_conf)\n    gcs_archive_bucket_name = _get_default_or_from_dag_run(GCS_ARCHIVE_BUCKET_NAME, DAG_PARAM_GCS_ARCHIVE_BUCKET_NAME,\n                                                           dag_run_conf)\n    gcs_archive_bucket_path = _get_default_or_from_dag_run(GCS_ARCHIVE_BUCKET_PATH, DAG_PARAM_GCS_ARCHIVE_BUCKET_PATH,\n                                                           dag_run_conf)\n    gcs_staging_bucket_name = _get_default_or_from_dag_run(GCS_STAGING_BUCKET_NAME, DAG_PARAM_GCS_STAGING_BUCKET_NAME,\n                                                           dag_run_conf)\n    gcs_staging_bucket_path = _get_default_or_from_dag_run(GCS_STAGING_BUCKET_PATH, DAG_PARAM_GCS_STAGING_BUCKET_PATH,\n                                                           dag_run_conf)\n    num_past_hours = _get_default_or_from_dag_run(DEFAULT_NUM_HOURS, DAG_PARAM_NUM_PAST_HOURS, dag_run_conf)\n\n\n    logging.info(f\"Landing bucket name [{gcs_landing_bucket_name}]\")\n    logging.info(f\"Landing bucket path [{gcs_landing_bucket_path}]\")\n    logging.info(f\"Staging bucket name [{gcs_staging_bucket_name}]\")\n    logging.info(f\"Staging bucket path [{gcs_staging_bucket_path}]\")\n    logging.info(f\"Archive bucket name [{gcs_archive_bucket_name}]\")\n    logging.info(f\"Archive bucket path [{gcs_archive_bucket_path}]\")\n    logging.info(f\"Num past hours [{num_past_hours}]\")\n\n    if dag_run_conf is not None and DAG_PARAM_RUN_ID in dag_run_conf:\n        run_id = dag_run_conf[DAG_PARAM_RUN_ID]\n        reprocessing_flag = True\n        logging.info(f\"Using RUN ID from user [{run_id}]\")\n    else:\n        run_id = f\"{execution_dt_utc.year:04}{execution_dt_utc.month:02}{execution_dt_utc.day:02}{execution_dt_utc.hour:02}{execution_dt_utc.minute:02}\"\n        reprocessing_flag = False\n        logging.info(f\"Creating new RUN ID [{run_id}]\")\n\n    gcs_run_path = f\"{gcs_staging_bucket_path}/{run_id}\"\n    gcs_run_full_path = f\"{gcs_staging_bucket_name}/{gcs_run_path}\"\n    logging.info(f\"Full RUN path on GCS [{gcs_run_full_path}]\")\n\n    job_name = f\"{DF_JOB_NAME}-{run_id}\"\n    logging.info(f\"Job name [{job_name}]\")\n\n    task_instance.xcom_push(key=TASK_PARAM_LANDING_BUCKET_NAME, value=gcs_landing_bucket_name)\n    task_instance.xcom_push(key=TASK_PARAM_LANDING_BUCKET_PATH, value=gcs_landing_bucket_path)\n    task_instance.xcom_push(key=TASK_PARAM_ARCHIVE_BUCKET_NAME, value=gcs_archive_bucket_name)\n    task_instance.xcom_push(key=TASK_PARAM_ARCHIVE_BUCKET_PATH, value=gcs_archive_bucket_path)\n    task_instance.xcom_push(key=TASK_PARAM_RUN_BUCKET_NAME, value=gcs_staging_bucket_name)\n    task_instance.xcom_push(key=TASK_PARAM_RUN_BUCKET_PATH, value=gcs_run_path)\n    task_instance.xcom_push(key=TASK_PARAM_RUN_PATH_FULL, value=gcs_run_full_path)\n    task_instance.xcom_push(key=TASK_PARAM_REPROCESSING, value=reprocessing_flag)\n    task_instance.xcom_push(key=TASK_PARAM_JOB_NAME, value=job_name)\n    task_instance.xcom_push(key=TASK_PARAM_NUM_PAST_HOURS, value=num_past_hours)","annotations":[]},{"name":"_is_reprocessing","returnType":"","parameters":[],"body":"def _is_reprocessing(**context):\n    reprocess_flag = context['ti'].xcom_pull(key=TASK_PARAM_REPROCESSING, task_ids=TASK_SETUP_PROCESSING)\n    logging.info(f\"Is reprocessing required [{reprocess_flag}]\")\n    return TASK_PREPARE_STAGING if not reprocess_flag else TASK_SKIP_PREPARE_STAGING","annotations":[]},{"name":"_get_gcs_files","returnType":"","parameters":["bucket_name","gcs_path"],"body":"def _get_gcs_files(bucket_name: str, gcs_path: str, **context):\n    logging.info(\n        f\"Listing files in [{bucket_name}/{gcs_path}]...\")\n    gcs_list_objects = GCSListObjectsOperator(\n        task_id='list_gcs_files' + str(uuid.uuid4()),\n        bucket=bucket_name,\n        prefix=gcs_path,\n        match_glob=\"**/*\" + DELIMITER,\n        impersonation_chain=None if TEST_RUN == \"true\" else DATAFLOW_SERVICE_ACCOUNT,\n        dag=dag\n    )\n    files = gcs_list_objects.execute(context)\n    logging.info(\n        f\"Found [{len(files)}] files in [{bucket_name}/{gcs_path}]\")\n    return files","annotations":[]},{"name":"_move_landing_to_staging","returnType":"","parameters":["source_bucket","source_objects","destination_bucket","destination_path","source_path"],"body":"def _move_landing_to_staging(source_bucket: str, source_objects: list, destination_bucket: str, destination_path: str,\n                             source_path: str, **context):\n    logging.info(\n        f\"Moving [{len(source_objects)}] landing files from [{source_bucket}/{source_path}] to staging [{destination_bucket}/{destination_path}]...\")\n    _move_blobs(source_bucket, source_objects, destination_bucket, destination_path, **context)\n    logging.info(\n        f\"Moved [{len(source_objects)}] landing files from [{source_bucket}/{source_path}] to staging [{destination_bucket}/{destination_path}]\")","annotations":[]},{"name":"_move_to_archive","returnType":"","parameters":[],"body":"def _move_to_archive(**context):\n    ti = context['ti']\n    gcs_archive_bucket_name = ti.xcom_pull(key=TASK_PARAM_ARCHIVE_BUCKET_NAME, task_ids=TASK_SETUP_PROCESSING)\n    gcs_archive_bucket_path = ti.xcom_pull(key=TASK_PARAM_ARCHIVE_BUCKET_PATH, task_ids=TASK_SETUP_PROCESSING)\n    gcs_run_bucket_name = ti.xcom_pull(key=TASK_PARAM_RUN_BUCKET_NAME, task_ids=TASK_SETUP_PROCESSING)\n    gcs_run_bucket_path = ti.xcom_pull(key=TASK_PARAM_RUN_BUCKET_PATH, task_ids=TASK_SETUP_PROCESSING)\n\n    logging.info(f\"Archive bucket name [{gcs_archive_bucket_name}]\")\n    logging.info(f\"Archive bucket path [{gcs_archive_bucket_path}]\")\n    logging.info(f\"Run bucket name [{gcs_run_bucket_name}]\")\n    logging.info(f\"Run bucket path [{gcs_run_bucket_path}]\")\n\n    logging.info(\n        f\"Archiving files from [{gcs_run_bucket_name}/{gcs_run_bucket_path}] to [{gcs_archive_bucket_name}/{gcs_archive_bucket_path}]\")\n\n    processed_files = _get_gcs_files(gcs_run_bucket_name, gcs_run_bucket_path, **context)\n    logging.info(f\"Found [{len(processed_files)}] processed files in [{gcs_run_bucket_name}/{gcs_run_bucket_path}]\")\n\n    if len(processed_files) == 0:\n        logging.error(f\"No files in run path [{gcs_run_bucket_name}/{gcs_run_bucket_path}]\")\n        raise AirflowFailException(f\"No files in run path [{gcs_run_bucket_name}/{gcs_run_bucket_path}]\")\n\n    per_hour_grouping = group_partitions_per_hour(processed_files, gcs_run_bucket_path + \"/\")\n\n    for partition in per_hour_grouping:\n        files_per_partition = per_hour_grouping[partition]\n        logging.info(\n            f\"Moving files from partition [{partition}] to [{gcs_archive_bucket_name}/{gcs_archive_bucket_path}/{partition}/]\")\n        _move_blobs(gcs_run_bucket_name, files_per_partition, gcs_archive_bucket_name,\n                    f\"{gcs_archive_bucket_path}/{partition}\", **context)\n    logging.info(f\"Processed [{len(per_hour_grouping)}] partitions\")","annotations":[]},{"name":"_prepare_staging","returnType":"","parameters":[],"body":"def _prepare_staging(**context):\n    ti = context['ti']\n    gcs_landing_bucket_name = ti.xcom_pull(key=TASK_PARAM_LANDING_BUCKET_NAME, task_ids=TASK_SETUP_PROCESSING)\n    gcs_landing_bucket_path = ti.xcom_pull(key=TASK_PARAM_LANDING_BUCKET_PATH, task_ids=TASK_SETUP_PROCESSING)\n    gcs_run_bucket_name = ti.xcom_pull(key=TASK_PARAM_RUN_BUCKET_NAME, task_ids=TASK_SETUP_PROCESSING)\n    gcs_run_bucket_path = ti.xcom_pull(key=TASK_PARAM_RUN_BUCKET_PATH, task_ids=TASK_SETUP_PROCESSING)\n    num_past_hours = ti.xcom_pull(key=TASK_PARAM_NUM_PAST_HOURS, task_ids=TASK_SETUP_PROCESSING)\n\n    dag_vars = get_dag_vars()\n    #num_past_hours = get_dag_var_as_int(VAR_AS_NUM_PAST_HOURS, dag_vars)\n    max_files = get_dag_var_as_int(VAR_AS_MAX_FILES, dag_vars)\n\n    logging.info(f\"Landing bucket name [{gcs_landing_bucket_name}]\")\n    logging.info(f\"Landing bucket path [{gcs_landing_bucket_path}]\")\n    logging.info(f\"Run bucket name [{gcs_run_bucket_name}]\")\n    logging.info(f\"Run bucket path [{gcs_run_bucket_path}]\")\n    logging.info(f\"Num past hours [{num_past_hours}]\")\n    logging.info(f\"Max files [{max_files}]\")\n\n    logging.info(f\"Getting landing files...\")\n    landing_files = _get_gcs_files(gcs_landing_bucket_name, gcs_landing_bucket_path, **context)\n    logging.info(f\"Got [{len(landing_files)}] landing files\")\n\n    all_selected_landing_files = select_files(landing_files, gcs_landing_bucket_path + \"/\", int(num_past_hours))\n    logging.info(f\"Selected [{len(all_selected_landing_files)}] landing files\")\n    if 0 < max_files < len(all_selected_landing_files):\n        selected_landing_files = all_selected_landing_files[:max_files]\n    else:\n        selected_landing_files = all_selected_landing_files\n    logging.info(f\"Limited selected landing files to [{len(selected_landing_files)}]\")\n    size =len(selected_landing_files)\n    if len(selected_landing_files) == 0:\n        create_custom_metrics(f\"pnr_input_file_count{SUFFIX}\", COMPOSER_PROJECT_ID, \"cloud_composer_workflow\",\n                              \"int64_value\", 0)\n        logging.error(f\"No files in landing [{gcs_landing_bucket_name}/{gcs_landing_bucket_path}]\")\n        raise AirflowFailException(f\"No files in landing [{gcs_landing_bucket_name}/{gcs_landing_bucket_path}]\")\n\n    _move_landing_to_staging(gcs_landing_bucket_name, selected_landing_files, gcs_run_bucket_name, gcs_run_bucket_path,\n                             gcs_landing_bucket_path, **context)\n    create_custom_metrics(f\"pnr_input_file_count{SUFFIX}\", COMPOSER_PROJECT_ID, \"cloud_composer_workflow\",\n                          \"int64_value\", size)","annotations":[]},{"name":"_process_staging","returnType":"","parameters":[],"body":"def _process_staging(**context):\n    logging.info(f\"Preparing Dataflow job...\")\n    dag = context[\"dag\"]\n    ti = context['ti']\n    job_name = ti.xcom_pull(key=TASK_PARAM_JOB_NAME, task_ids=TASK_SETUP_PROCESSING)\n    run_path = ti.xcom_pull(key=TASK_PARAM_RUN_PATH_FULL, task_ids=TASK_SETUP_PROCESSING)\n\n    dag_vars = get_dag_vars()\n    num_workers = get_dag_var_as_int(VAR_AS_DF_NUM_WORKERS, dag_vars)\n    max_workers = get_dag_var_as_int(VAR_AS_DF_MAX_WORKERS, dag_vars)\n    machine_type = get_dag_var_as_string(VAR_AS_DF_MACHINE_TYPE, dag_vars)\n\n    logging.info(f\"Num workers [{num_workers}]\")\n    logging.info(f\"Max workers [{max_workers}]\")\n    logging.info(f\"Machine type [{machine_type}]\")\n\n    df_job = DataflowTemplatedJobStartOperator(\n        task_id=TASK_TRIGGER_DF_JOB,\n        template=DATAFLOW_TEMPLATE_LOCATION,\n        project_id=PIPELINE_PROJECT_ID,\n        job_name=job_name,\n        location=DATAFLOW_REGION,\n        environment={\n            \"serviceAccountEmail\": DATAFLOW_SERVICE_ACCOUNT,\n            \"additionalUserLabels\": USER_LABELS\n        },\n        parameters={\n            \"runPath\": f\"gs://{run_path}/*{DELIMITER}\"\n        },\n        dataflow_default_options={\n            \"tempLocation\": DATAFLOW_TEMP_LOCATION,\n            \"numWorkers\": num_workers,\n            \"maxWorkers\": max_workers,\n            \"machineType\": machine_type\n        },\n        dag=dag\n    )\n    retry_option = 0\n    while retry_option < MAX_RETRIES:\n        try:\n            retry_option += 1\n            logging.info(f\"Try [{retry_option}/{MAX_RETRIES}]\")\n            logging.info(f\"Starting Dataflow job [{job_name}]...\")\n            df_job.execute(context)\n            break\n        except (Exception,HttpError) as err:\n            logging.error(\n                f\"DataflowTemplatedJobStartOperator got HttpError exception while running DF job [{err}]\")\n            create_custom_metrics(f\"pnr_processed_file_count{SUFFIX}\", COMPOSER_PROJECT_ID, \"cloud_composer_workflow\",\n                                  \"int64_value\", 0)\n            raise AirflowFailException(f\"Unable to start Dataflow job after [{MAX_RETRIES}]\")\n            time.sleep(5)\n\n            continue\n        break\n    logging.info(f\"Finished Dataflow job [{job_name}]\")\n    ti.xcom_push(\"job_status\", value=\"JOB_STATE_DONE\")\n    pnr_processed_file_count = get_valid_record_count(job_name, \"pnr_valid_records\")\n    logging.info(f\"pnr_processed_file_count [{pnr_processed_file_count}]\")\n    create_custom_metrics(f\"pnr_processed_file_count{SUFFIX}\", COMPOSER_PROJECT_ID, \"cloud_composer_workflow\",\n                              \"int64_value\", pnr_processed_file_count)\n    diff_in_mins = get_time_diff_between_processed_tm_vs_current_time()\n    logging.info(f\"Time difference between processed_dt and current time is [{diff_in_mins}]\")\n    create_custom_metrics(f\"pnr_processed_time_lag_in_mins{SUFFIX}\", COMPOSER_PROJECT_ID, \"cloud_composer_workflow\",\n                              \"int64_value\", diff_in_mins)","annotations":[]},{"name":"get_time_diff_between_processed_tm_vs_current_time","returnType":"","parameters":[],"body":"def get_time_diff_between_processed_tm_vs_current_time():\n    client = get_bq_impersonated_client(service_account_name, PIPELINE_PROJECT_ID)\n    try:\n        time_difference = \"\"\"\n                                SELECT\n                                    MAX(ingest_ts) max_ingest_ts,\n                                    CURRENT_TIMESTAMP() AS current_ts,\n                                    TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), MAX(ingest_ts), MINUTE) diff_in_mins\n                                FROM\n                                  {0}.{1}.{2}  WHERE ingest_ts is not null\n                                \"\"\".format(PIPELINE_PROJECT_ID, FRESHNESS_CHECK_DATASET_NAME, FRESHNESS_CHECK_TABLE_NAME)\n\n        logging.info(f\"time_difference query: {time_difference}\")\n        query_job = client.query(time_difference, job_config=bigquery.QueryJobConfig(\n            labels={\"pnr-data-ingestion-label\": billing_label}), job_id_prefix=billing_label)\n        query_job.result()\n        records_dict = [dict(row) for row in query_job]\n        max_ingest_ts_in_batch = records_dict[0]['max_ingest_ts']\n        current_ts = records_dict[0]['current_ts']\n        diff_in_mins = records_dict[0]['diff_in_mins']\n        logging.info(f\"max_ingest_ts_in_batch: {max_ingest_ts_in_batch}\")\n        logging.info(f\"current_ts: {current_ts}\")\n        logging.info(f\"diff_in_mins: {diff_in_mins}\")\n\n    except Exception as e:\n        return logging.info('Error: {}'.format(str(e)))\n\n    return diff_in_mins","annotations":[]},{"name":"_cleanup_xcom","returnType":"","parameters":["context","session"],"body":"def _cleanup_xcom(context, session=None):\n    dag_id = context[\"dag\"].dag_id\n    session.query(XCom).filter(XCom.dag_id == dag_id).delete()","annotations":["provide_session"]}],"function_dependencies":{"_get_df_service":["build"],"list_jobs":["_get_df_service"],"get_job_id":["list_jobs"],"get_job_status":["list_jobs"],"get_job_metrics":["_get_df_service"],"get_valid_record_count":["get_job_id","get_job_metrics"],"create_custom_metrics":["get_metric_service_client","int","str"],"get_bq_impersonated_client":["getImpersonatedCredentials"],"_get_storage_client":["_get_impersonated_credentials"],"_move_blobs":["open","round","enumerate","str","len","BashOperator"],"_setup_processing":["_get_default_or_from_dag_run"],"_get_gcs_files":["GCSListObjectsOperator","len","str"],"_move_landing_to_staging":["_move_blobs","len"],"_move_to_archive":["_get_gcs_files","AirflowFailException","_move_blobs","len","group_partitions_per_hour"],"_prepare_staging":["get_dag_vars","_move_landing_to_staging","_get_gcs_files","AirflowFailException","int","get_dag_var_as_int","create_custom_metrics","select_files","len"],"_process_staging":["DataflowTemplatedJobStartOperator","get_dag_vars","get_dag_var_as_string","AirflowFailException","get_valid_record_count","get_dag_var_as_int","get_time_diff_between_processed_tm_vs_current_time","create_custom_metrics"],"get_time_diff_between_processed_tm_vs_current_time":["str","get_bq_impersonated_client","dict"]},"DAG":["setup_processing_task >> is_reprocessing_task >> [prepare_staging_task, skip_prepare_staging_task] >> process_staging_task >> move_to_archive_task"]}  , "demo/src/main/DAG.py ": 
{"imports":["airflow.DAG","airflow.operators.python_operator.PythonOperator","datetime.datetime","datetime.timedelta","typing.Callable"],"classes":[{"name":"BaseTask","extendedClasses":[],"annotations":[],"methods":[{"name":"__init__","returnType":"None","parameters":["self","task_id","python_callable"],"body":"    def __init__(self, task_id: str, python_callable: Callable) -> None:\n        self.task_id = task_id\n        self.python_callable = python_callable","annotations":[]},{"name":"create_task","returnType":"PythonOperator","parameters":["self","dag"],"body":"    def create_task(self, dag: DAG) -> PythonOperator:\n        return PythonOperator(\n            task_id=self.task_id,\n            python_callable=self.python_callable,\n            dag=dag,\n        )","annotations":[]}]},{"name":"DataTask","extendedClasses":["BaseTask"],"annotations":[],"methods":[{"name":"__init__","returnType":"None","parameters":["self","task_id","data"],"body":"    def __init__(self, task_id: str, data: str) -> None:\n        super().__init__(task_id, self.process_data)\n        self.data = data","annotations":[]},{"name":"process_data","returnType":"None","parameters":["self"],"body":"    def process_data(self) -> None:\n        print(f\" Processing data tas k {self.task_id} with data: {self.data}\")","annotations":[]}]},{"name":"ComputeTask","extendedClasses":["BaseTask"],"annotations":[],"methods":[{"name":"__init__","returnType":"None","parameters":["self","task_id","operation"],"body":"    def __init__(self, task_id: str, operation: str) -> None:\n        super().__init__(task_id, self.process_compute)\n        self.operation = operation","annotations":[]},{"name":"process_compute","returnType":"None","parameters":["self"],"body":"    def process_compute(self) -> None:\n        print(f\"Processing compute task {self.task_id} with operation: {self.operation}\")","annotations":[]}]}],"functions":[],"function_dependencies":{"create_task":["PythonOperator"],"__init__":["super"],"process_data":["print"],"process_compute":["print"]},"DAG":["data_task2_op >> compute_task_op","data_task1_op >> compute_task_op"]}  , "demo/src/main/java/Cube.java ": 
{"imports":["java.util"],"classes":[{"name":"Cube","annotations":[],"extendedClass":"","implementedInterfaces":["Cloneable"],"fields":[{"name":"nextEdgePos","type":"Map<Character, EdgePos>","annotations":[]},{"name":"nextCornerPos","type":"Map<Character, CornerPos>","annotations":[]},{"name":"nextEdgeOrientation","type":"Map<Character, List<Map<Byte, Byte>>>","annotations":[]},{"name":"nextCornerOrientation","type":"Map<Character, List<Map<Byte, Byte>>>","annotations":[]},{"name":"edgeList","type":"byte[][]","annotations":[]},{"name":"cornerList","type":"byte[][]","annotations":[]},{"name":"binEncoding","type":"Map<Character, Byte>","annotations":[]},{"name":"priority","type":"Map<Character, Byte>","annotations":[]},{"name":"edgeNumberForPos","type":"Map<Byte, Byte>","annotations":[]},{"name":"cornerNumberForPos","type":"Map<Byte, Byte>","annotations":[]},{"name":"edgePossiblePlacesStage3","type":"int[][]","annotations":[]},{"name":"cornerPossiblePlacesStage3","type":"int[][]","annotations":[]},{"name":"edge","type":"Edge","annotations":[]},{"name":"corner","type":"Corner","annotations":[]}],"methods":[{"name":"clone","returnType":"Cube","annotations":["Override"],"parameters":[],"body":"{\n    return new Cube(this.getEdge().clone(), this.getCorner().clone());\n}"},{"name":"execute","returnType":"Cube","annotations":[],"parameters":["Cube c","String s"],"body":"{\n    Cube temp = c.clone();\n    String[] moves = s.split(\" \");\n    if (moves.length > 1) {\n        StringBuilder sBuilder = new StringBuilder();\n        for (String string : moves) {\n            if (string.length() == 1)\n                sBuilder.append(string.charAt(0));\n            else if (string.charAt(1) == '2')\n                sBuilder.append(String.valueOf(string.charAt(0)).repeat(2));\n            else\n                sBuilder.append(String.valueOf(string.charAt(0)).repeat(3));\n        }\n        s = sBuilder.toString();\n    }\n    for (int i = 0; i < s.length(); i++) {\n        char ch = s.charAt(i);\n        EdgePos edgePos = temp.getEdge().getEdgePos().clone();\n        EdgeOrientation edgeOrientation = temp.getEdge().getEdgeOrientation().clone();\n        for (int j = 0; j < 12; j++) {\n            edgeOrientation.setVal(j, nextEdgeOrientation.get(ch).get(edgePos.getVal()[j]).get(edgeOrientation.getVal()[j]));\n            edgePos.setVal(j, nextEdgePos.get(ch).getVal()[edgePos.getVal()[j]]);\n        }\n        temp.setEdge(new Edge(edgePos, edgeOrientation));\n        CornerPos cornerPos = temp.getCorner().getCornerPos().clone();\n        CornerOrientation cornerOrientation = temp.getCorner().getCornerOrientation().clone();\n        for (int j = 0; j < 8; j++) {\n            cornerOrientation.setVal(j, nextCornerOrientation.get(ch).get(cornerPos.getVal()[j]).get(cornerOrientation.getVal()[j]));\n            cornerPos.setVal(j, nextCornerPos.get(ch).getVal()[cornerPos.getVal()[j]]);\n        }\n        temp.setCorner(new Corner(cornerPos, cornerOrientation));\n    }\n    return temp;\n}"},{"name":"reverseAlgorithm","returnType":"String","annotations":[],"parameters":["String s"],"body":"{\n    StringBuilder result = new StringBuilder();\n    for (int i = 0; i < s.length(); i++) result.append(String.valueOf(s.charAt(i)).repeat(3));\n    return new StringBuilder(result.toString()).reverse().toString();\n}"},{"name":"getAlgorithm","returnType":"ArrayList<String>","annotations":[],"parameters":["String moves"],"body":"{\n    class Temp {\n\n        final char ch;\n\n        final byte b;\n\n        public Temp(char ch, byte b) {\n            this.ch = ch;\n            this.b = b;\n        }\n    }\n    Stack<Temp> s = new Stack<>();\n    ArrayList<String> v = new ArrayList<>(Arrays.asList(\"\", \"\", \"2\", \"'\"));\n    ArrayList<String> result = new ArrayList<>();\n    for (int i = 0; i < moves.length(); i++) {\n        if (s.isEmpty() || s.peek().ch != moves.charAt(i))\n            s.push(new Temp(moves.charAt(i), (byte) 1));\n        else {\n            Temp x = s.pop();\n            if (x.b != (byte) 3)\n                s.push(new Temp(x.ch, (byte) (x.b + 1)));\n        }\n    }\n    while (!s.isEmpty()) {\n        Temp x = s.pop();\n        if (x.b != 0)\n            result.add(0, x.ch + v.get(x.b));\n    }\n    return result;\n}\n[Local class: Temp]"},{"name":"toString","returnType":"String","annotations":["Override"],"parameters":[],"body":"{\n    return \"Cube{\\n\" + \"edge=\" + edge.toString() + \",\\ncorner=\" + corner.toString() + \"\\n}\";\n}"},{"name":"getEdge","returnType":"Edge","annotations":[],"parameters":[],"body":"{\n    return edge;\n}"},{"name":"setEdge","returnType":"void","annotations":[],"parameters":["Edge edge"],"body":"{\n    this.edge = edge;\n}"},{"name":"getCorner","returnType":"Corner","annotations":[],"parameters":[],"body":"{\n    return corner;\n}"},{"name":"setCorner","returnType":"void","annotations":[],"parameters":["Corner corner"],"body":"{\n    this.corner = corner;\n}"}],"nestedClasses":[{"name":"Temp","annotations":[],"extendedClass":"","implementedInterfaces":[],"fields":[{"name":"ch","type":"char","annotations":[]},{"name":"b","type":"byte","annotations":[]}],"methods":[],"nestedClasses":[]}]}]}  , "demo/src/main/java/com/example/demo/DemoApplication.java ": 
{"imports":["org.springframework.boot.SpringApplication","org.springframework.boot.autoconfigure.SpringBootApplication"],"classes":[{"name":"DemoApplication","annotations":["SpringBootApplication"],"extendedClass":"","implementedInterfaces":[],"fields":[],"methods":[{"name":"main","returnType":"void","annotations":[],"parameters":["String[] args"],"body":"{\n    SpringApplication.run(DemoApplication.class, args);\n}"}],"nestedClasses":[]}]}  , "demo/src/test/java/com/example/demo/DemoApplicationTests.java ": 
{"imports":["org.junit.jupiter.api.Test","org.springframework.boot.test.context.SpringBootTest"],"classes":[{"name":"DemoApplicationTests","annotations":["SpringBootTest"],"extendedClass":"","implementedInterfaces":[],"fields":[],"methods":[{"name":"contextLoads","returnType":"void","annotations":["Test"],"parameters":[],"body":"{\n}"}],"nestedClasses":[]}]}} 
